{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ml_final_demo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQ4WkLQRCmyA",
        "colab_type": "text"
      },
      "source": [
        "**Step 1**\n",
        "Note : Skip if not need \n",
        "\n",
        "only run when google drive need to mount drive\n",
        "Mount Google drive to retrive data, save model weight and save results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHgeHq_xdU-i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiDy5wkCC8SI",
        "colab_type": "text"
      },
      "source": [
        "**Step 2 :** \n",
        "\n",
        "TPU model only avalible tensorflow 1.13.1 before version"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDh_34BbQrzQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install tensorflow==1.13.1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYXPcG167LVQ",
        "colab_type": "text"
      },
      "source": [
        "**Step 3 :** \n",
        "\n",
        "Import nessacery pacege for code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hr1RQveQQ9Fn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To run and train model use below command \n",
        "# python3 train_model_ml.py  '-m' 'sl' '-e' '10' '-b' '256' '-alpha' '0.1' '-beta' '1.0' '-c' 'C5'\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import print_function\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow.keras.backend as K\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.layers import Input,Reshape, Conv1D, Dense, MaxPooling1D, Dropout, Flatten, Activation, BatchNormalization\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import Model, layers\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from keras.utils import np_utils\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, EarlyStopping, TensorBoard, ReduceLROnPlateau\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
        "from sklearn.metrics import f1_score\n",
        "from tensorflow.keras.callbacks import Callback, LearningRateScheduler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HD__QEZVDhEO",
        "colab_type": "text"
      },
      "source": [
        "**Step 4:**\n",
        "\n",
        "Assign Dataset name to A or B to run model on that dataset\n",
        "\n",
        "Set path for train and test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obmQoqbHe3dD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_name = 'A'\n",
        "# drive_path_main_folder = '/content/drive/My Drive/ML_Project'\n",
        "X_Train_Data_Sample_Path = r'%s_train_samples.xlsx'% (dataset_name)\n",
        "Y_Train_Data_label_Path = r'%s_train_labels.xlsx'% (dataset_name)\n",
        "\n",
        "X_Test_Data_Loaded_Path = r'%s_test_samples.xlsx'% (dataset_name)\n",
        "Y_Test_Data_label_Path = r'%s_test_labels.xlsx'% (dataset_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_D0qZQrDouX",
        "colab_type": "text"
      },
      "source": [
        "**Step 5 :**\n",
        "\n",
        "Read data from excel to local variable to use it future for different column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgaCn1Kge_SF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_Train_data_loaded = pd.read_excel(X_Train_Data_Sample_Path)\n",
        "Y_Train_data_loaded = pd.read_excel(Y_Train_Data_label_Path)\n",
        "X_Test_Data_Loaded = pd.read_excel(X_Test_Data_Loaded_Path)\n",
        "Y_Test_data_loaded = pd.read_excel(Y_Test_Data_label_Path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oe31dOJ78Pp5",
        "colab_type": "text"
      },
      "source": [
        "**Step 6 :**\n",
        "\n",
        "Set parameter like column you want to run, batch size, number of epoch, alpha, beta (for loss function), split percentage for validation and train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "535JvrXsDGX9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "column_name = 'B'\n",
        "num_classes = Y_Test_data_loaded['Labels'].max()\n",
        "model_name ='sl'\n",
        "batch_size = 256\n",
        "if dataset_name == \"A\":\n",
        "  epochs = 40\n",
        "else: \n",
        "  epochs = 20\n",
        "split_per = 0.2\n",
        "num_of_counter = 4\n",
        "alpha = 0.01\n",
        "beta = 0.1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsVZtlOMDxp2",
        "colab_type": "text"
      },
      "source": [
        "**Step 7 :**\n",
        "\n",
        "Data proeccsing using Selected column for lable and formate dataset to run on tpu model\n",
        "\n",
        "Train Sample data proecssing:\n",
        "\n",
        "\n",
        "1.   Find Mean for X train samples \n",
        "2.   Substract mean from X train samples\n",
        "3.   Transfrom data between 0 to 1 using MinMaxScaler\n",
        "4.   Split data for Validation\n",
        "5. Transfrom lable to 0 and 1 using to_categorical\n",
        "6. Reshape X train sample (because model accept that way\n",
        "7. Repet Step 2 to 6 for test data sample and lable \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwJd8xnLmNVk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Xm = X_Train_data_loaded\n",
        "Y = Y_Train_data_loaded-1\n",
        "means = Xm.mean(axis=0)\n",
        "Xm = (Xm - means)\n",
        "x_scaler = MinMaxScaler()\n",
        "X_train = x_scaler.fit_transform(Xm)\n",
        "X_train, x_validation, y_train, y_validation = train_test_split(X_train,Y,test_size = split_per,train_size = 1 - split_per)\n",
        "\n",
        "y_train = pd.DataFrame(y_train[column_name])\n",
        "y_train = np_utils.to_categorical(y_train,num_classes)\n",
        "\n",
        "y_validation = pd.DataFrame(y_validation[\"B\"]).values\n",
        "y_validation = np_utils.to_categorical(y_validation,num_classes)\n",
        "\n",
        "input_features = X_train.shape[1]\n",
        "seq_len = X_train.shape[0]\n",
        "\n",
        "X_train = np.array(X_train).reshape(seq_len, input_features, 1)\n",
        "x_validation = np.array(x_validation).reshape(x_validation.shape[0], input_features, 1)\n",
        "\n",
        "X_T = X_Test_Data_Loaded - means\n",
        "Y_T = Y_Test_data_loaded['Labels'].values-1\n",
        "\n",
        "X_test = x_scaler.transform(X_T)\n",
        "y_test = np_utils.to_categorical(Y_T,num_classes)\n",
        "X_test = np.array(X_test).reshape(X_test.shape[0], input_features, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXwNFni2A4V2",
        "colab_type": "text"
      },
      "source": [
        "**Step 8:** \n",
        "\n",
        "Build class for logging data while traing model also, track and save accuracy and loss while running code\n",
        "\n",
        "also class for tracking SGD learing rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4gPT-BGtXwo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class LoggerCallback(Callback):\n",
        "    \"\"\"\n",
        "    Log train/val loss and acc into file for later plots.\n",
        "    \"\"\"\n",
        "    def __init__(self, model, X_train, y_train,  model_name, epochs, alpha, beta):\n",
        "        super(LoggerCallback, self).__init__()\n",
        "        self.model = model\n",
        "        self.X_train = X_train\n",
        "        self.y_train = y_train\n",
        "        self.n_class = y_train.shape[1]\n",
        "        self.model_name = model_name\n",
        "        self.epochs = epochs\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "\n",
        "        self.train_loss = []\n",
        "        self.test_loss = []\n",
        "        self.train_acc = []\n",
        "        self.test_acc = []\n",
        "        self.train_loss_class = [None]*self.n_class\n",
        "        self.train_acc_class = [None]*self.n_class\n",
        "\n",
        "        # the followings are used to estimate LID\n",
        "        self.lid_k = 20\n",
        "        self.lid_subset = 128\n",
        "        self.lids = []\n",
        "\n",
        "        # complexity - Critical Sample Ratio (csr)\n",
        "        self.csr_subset = 500\n",
        "        self.csr_batchsize = 100\n",
        "        self.csrs = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        tr_acc = logs.get('acc')\n",
        "        tr_loss = logs.get('loss')\n",
        "        val_loss = logs.get('val_loss')\n",
        "        val_acc = logs.get('val_acc')\n",
        "\n",
        "        self.train_loss.append(tr_loss)\n",
        "        self.test_loss.append(val_loss)\n",
        "        self.train_acc.append(tr_acc)\n",
        "        self.test_acc.append(val_acc)\n",
        "\n",
        "        print('ALL acc:', self.test_acc)\n",
        "\n",
        "        file_name = 'loss_%s_%s' % (self.model_name, self.alpha)\n",
        "        np.save(file_name, np.stack((np.array(self.train_loss), np.array(self.test_loss))))\n",
        "        file_name = 'acc_%s_%s' % (self.model_name, self.alpha)\n",
        "        np.save(file_name, np.stack((np.array(self.train_acc), np.array(self.test_acc))))\n",
        "\n",
        "        return\n",
        "        \n",
        "class SGDLearningRateTracker(Callback):\n",
        "    def __init__(self, model):\n",
        "        super(SGDLearningRateTracker, self).__init__()\n",
        "        self.model = model\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        init_lr = float(K.get_value(self.model.optimizer.lr))\n",
        "        decay = float(K.get_value(self.model.optimizer.decay))\n",
        "        iterations = float(K.get_value(self.model.optimizer.iterations))\n",
        "        lr = init_lr * (1. / (1. + decay * iterations))\n",
        "        print('init lr: %.4f, current lr: %.4f, decay: %.4f, iterations: %s' % (init_lr, lr, decay, iterations))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44cZukFSBU-1",
        "colab_type": "text"
      },
      "source": [
        "**Step 9:** \n",
        "\n",
        "Symantric cross entropy for model\n",
        "which goal of this project using this loss function train model with noisy lable data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDEz9ie2tGvS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def symmetric_cross_entropy(alpha, beta):\n",
        "    def loss(y_true, y_pred):\n",
        "        y_true_1 = y_true\n",
        "        y_pred_1 = y_pred\n",
        "\n",
        "        y_true_2 = y_true\n",
        "        y_pred_2 = y_pred\n",
        "\n",
        "        y_pred_1 = tf.clip_by_value(y_pred_1, 1e-7, 1.0)\n",
        "        y_true_2 = tf.clip_by_value(y_true_2, 1e-4, 1.0)\n",
        "\n",
        "        return alpha*tf.reduce_mean(-tf.reduce_sum(y_true_1 * tf.math.log(y_pred_1), axis = -1)) + beta*tf.reduce_mean(-tf.reduce_sum(y_pred_2 * tf.math.log(y_true_2), axis = -1))\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eR21d4XsBr5N",
        "colab_type": "text"
      },
      "source": [
        "**Step 10:**\n",
        "\n",
        "TPU model require data in batch size so it can achive parraller processing while training model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFHgkQ4lR3aD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_input_fn():\n",
        "    # convert the inputs to a Dataset.\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((X_train.astype(np.float32),y_train))\n",
        "    \n",
        "    dataset = dataset.shuffle(1000).repeat().batch(batch_size, drop_remainder=True)\n",
        "\n",
        "    # return the dataset.\n",
        "    return dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnSpk-DJB9yt",
        "colab_type": "text"
      },
      "source": [
        "**Step 11:**\n",
        "\n",
        "Decrease learing rate based on epoch so it can learn better"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-Wa6_DiOOO-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_lr_scheduler():\n",
        "    def scheduler(epoch):\n",
        "        if epoch < 20:\n",
        "            return 0.2\n",
        "        elif epoch < 35:\n",
        "            return 0.15\n",
        "        else:\n",
        "          return 0.1\n",
        "    return LearningRateScheduler(scheduler)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mKscAB6CKLR",
        "colab_type": "text"
      },
      "source": [
        "**Step 12:**\n",
        "\n",
        "Build model, Compile model, set callback for tracking, convert model to TPU model and train TPU model on dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hxkWFTZR5ld",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train():\n",
        "    print(\"seq_len\", seq_len, \"num_classes\", num_classes, \"input_features:\", input_features)\n",
        "    \n",
        "    # load model\n",
        "    model = get_model(num_classes)\n",
        "    model.summary()\n",
        "    \n",
        "    # Select Optimizer\n",
        "    optimizer = SGD(lr=0.15, decay=1e-4, momentum=0.9,nesterov=True)\n",
        "\n",
        "    # create loss\n",
        "    loss = symmetric_cross_entropy(alpha,beta)\n",
        "\n",
        "    # model\n",
        "    model.compile(\n",
        "        loss=loss,\n",
        "        optimizer=optimizer,\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    model_save_file = \"model_%s_{epoch:02d}_%s.hdf5\" % (model_name, column_name)\n",
        "    \n",
        "    ## do real-time updates using callbakcs\n",
        "    callbacks = []\n",
        "\n",
        "    cp_callback = ModelCheckpoint(model_save_file,\n",
        "                                    monitor='val_loss',\n",
        "                                    verbose=0,\n",
        "                                    save_best_only=True,\n",
        "                                    save_weights_only=True,\n",
        "                                    period=1)\n",
        "    callbacks.append(cp_callback)\n",
        "    \n",
        "    # learning rate scheduler if use sgd\n",
        "    lr_scheduler = get_lr_scheduler()\n",
        "    callbacks.append(lr_scheduler)\n",
        "    callbacks.append(SGDLearningRateTracker(model))\n",
        "    # learning rate scheduler if use sgd\n",
        "    lr_scheduler = get_lr_scheduler()\n",
        "    callbacks.append(lr_scheduler)\n",
        "\n",
        "    # acc, loss, lid\n",
        "    log_callback = LoggerCallback(model, X_train, y_train, model_name, epochs, alpha, beta)\n",
        "    callbacks.append(log_callback)\n",
        "    callback_early_stopping = EarlyStopping(monitor='val_loss', patience=15, verbose=1)\n",
        "    callbacks.append(callback_early_stopping)\n",
        "\n",
        "    # train model\n",
        "    try:\n",
        "        device_name = os.environ['COLAB_TPU_ADDR']\n",
        "        TPU_ADDRESS = 'grpc://' + device_name\n",
        "        print('Found TPU at: {}'.format(TPU_ADDRESS))\n",
        "    except KeyError:\n",
        "        print('TPU not found')\n",
        "  \n",
        "    tpu_model = tf.contrib.tpu.keras_to_tpu_model(\n",
        "        model,\n",
        "        strategy=tf.contrib.tpu.TPUDistributionStrategy(\n",
        "            tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS))\n",
        "    )\n",
        "\n",
        "    tpu_model.fit(train_input_fn,\n",
        "                    epochs=epochs,\n",
        "                    steps_per_epoch=int(len(X_train) / batch_size),\n",
        "                    validation_data = (x_validation, y_validation), \n",
        "                    validation_steps =int(len(x_validation) / batch_size),\n",
        "                    callbacks=callbacks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_xicUNYDJ1l",
        "colab_type": "text"
      },
      "source": [
        "**Step 13:**\n",
        "\n",
        "Build model, load weight from save model weight and then test model on test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OH4lzcUlR-Bd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(need_to_predict = False):\n",
        "    seq_len = X_test.shape[0]\n",
        "    # load model\n",
        "    model = get_model(num_classes)\n",
        "    \n",
        "    # Select Optimizer\n",
        "    optimizer = SGD(lr=0.1, decay=1e-4, momentum=0.9,nesterov=True)\n",
        "\n",
        "    # create loss\n",
        "    loss = symmetric_cross_entropy(alpha, beta)\n",
        "    \n",
        "    # model\n",
        "    model.load_weights(save_model_path)\n",
        "    \n",
        "    model.compile(\n",
        "        loss=loss,\n",
        "        optimizer=optimizer,\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    score = model.evaluate(X_test.astype(np.float32), y_test, verbose=0)\n",
        "    print(\"%s: %.2f%%\" %(model.metrics_names[1], score[1]*100))\n",
        "    print(\"%s: %.2f%%\" %(model.metrics_names[0], score[0]*100))\n",
        "\n",
        "    if need_to_predict:\n",
        "      y_pred = model.predict(X_test.astype(np.float32))\n",
        "\n",
        "      y_pred=np.argmax(y_pred, axis = 1)\n",
        "      y_test_=np.argmax(y_test, axis = 1)\n",
        "      f1_score_mesure = f1_score(y_test_, y_pred, average = 'weighted')\n",
        "\n",
        "      print(\"F measure \", f1_score_mesure)\n",
        "      return y_pred, f1_score_mesure, score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlea7H3mDYgX",
        "colab_type": "text"
      },
      "source": [
        "**Step 14:**\n",
        "\n",
        "Build model using Conv1D and 3 hidden layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNwJMy-mSAVZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_model(num_classes=10):\n",
        "    \n",
        "    inputs_data = Input(shape=(input_features,1))\n",
        "    b = Conv1D(3, 3, padding='same', kernel_initializer=\"he_normal\", name='block1_conv1')(inputs_data)\n",
        "    b = BatchNormalization()(b)\n",
        "    b = Activation('relu')(b)\n",
        "\n",
        "    b = Conv1D(3, 3, padding='same', kernel_initializer=\"he_normal\", name='block1_conv2')(b)\n",
        "    b = BatchNormalization()(b)\n",
        "    b = Activation('relu')(b)\n",
        "\n",
        "    b = MaxPooling1D(2, strides=2, name='block1_pool')(b)\n",
        "\n",
        "    b = Flatten(name='flatten')(b)\n",
        "\n",
        "    b = Dense(128)(b)\n",
        "    b = Activation('relu',name='lid')(b)\n",
        "    b = Dropout(0.2)(b)\n",
        "   \n",
        "    b = Dense(32)(b)\n",
        "    b = Activation('relu')(b)\n",
        "    b = Dropout(0.2)(b)\n",
        "\n",
        "    b = Dense(num_classes)(b)\n",
        "    outputs_lable = Activation('softmax')(b)\n",
        "    model = Model(inputs=inputs_data, outputs=outputs_lable)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kfxyCkLECT2",
        "colab_type": "text"
      },
      "source": [
        "**Step 15:**\n",
        "\n",
        "Train model, test model and see the results, predict value by train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2N8P3OxRaaxP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train()\n",
        "need_to_predict = True\n",
        "y_pred_obj = {}\n",
        "save_data_file_name = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "Save_File_Name = '%s_data_%s.csv' % (column_name, save_data_file_name)\n",
        "Save_File_Name_Text = 'Text_%s_data_%s.txt' % (column_name, save_data_file_name)\n",
        "counter = num_of_counter\n",
        "text_file = open(Save_File_Name_Text,\"w+\")\n",
        "for x in range(epochs):\n",
        "  x = epochs - x\n",
        "  if counter < 0:\n",
        "    break\n",
        "  if x < 10:\n",
        "    save_model = \"%s_%s%s_%s\" % (model_name,0,x, column_name)\n",
        "  else:\n",
        "    save_model = \"%s_%s_%s\" % (model_name,x, column_name)\n",
        "  save_model_path = \"model_%s.hdf5\" % (save_model)\n",
        "  if os.path.exists(save_model_path):\n",
        "    counter = counter - 1\n",
        "    print(save_model_path)\n",
        "    if need_to_predict:\n",
        "      y_pred, f1_score_mesure, score = test(True)\n",
        "      text_file.write(\"Accuracy %s\\r\\n\\n F measure %s\\r\\n\\n Score %s\\r\\n\\n\\n\\n\\n\" % (score[1]*100, f1_score_mesure,score ))\n",
        "      y_pred_obj.update( {save_model : y_pred + 1} )\n",
        "    else:\n",
        "      test()\n",
        "\n",
        "if need_to_predict:\n",
        "  text_file.close()\n",
        "  df = pd.DataFrame(y_pred_obj)\n",
        "  df.to_csv(Save_File_Name,index=False) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4XT9KAbFJNN",
        "colab_type": "text"
      },
      "source": [
        "**Step 16:**\n",
        "\n",
        "Just For Understaing\n",
        "this will return error in each column compare to B"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6IxNtqcRFId",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "column_array = ['B', 'C5', 'C10', 'C15', 'C20', 'C25', 'C30', 'C35', 'C40', 'A5', 'A10', 'A15', 'A20', 'A25', 'A30', 'A35', 'A40', 'N5', 'N10', 'N15', 'N20', 'N25', 'N30', 'N35', 'N40']\n",
        "for i in range(len(column_array) -1):\n",
        "  tempCount = i + 1\n",
        "  tempDifference = np.where((Y_Train_data_loaded['B'] != Y_Train_data_loaded[column_array[tempCount]]), 1, 0)\n",
        "  pddf = pd.DataFrame(tempDifference)\n",
        "  cdcs = pddf[0].values\n",
        "  print(column_array[tempCount] , (pddf[pddf.values == 1].shape[0]/ cdcs.shape[0])*100)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}